# -*- coding: utf-8 -*-
"""glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e-7CU_5Aa3whMcgmPiAI8T0ekK5y0ATr

Lets Create interesting NLP project

The most commonly used models for word embeddings are [word2vec](https://github.com/dav/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/) which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).

Word2Vec word embeddings are vector representations of words, 
that are typically learnt by an unsupervised model when fed 
with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that [distance](https://en.wikipedia.org/wiki/Euclidean_distance) between vectors for words with close meanings ("king" and "queen" for example) are closer than distance for words with complety different meanings ("king" and "carpet" for example).

![Замещающий текст](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)
Image from [developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)

Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:

**king - man + woman = queen**

word embedding method is Glove (“Global Vectors”). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.
"""

!wget https://nlp.stanford.edu/data/glove.42B.300d.zip #https://nlp.stanford.edu/data/glove.6B.zip

# !unzip /content/glove.6B.zip
!unzip /content/glove.42B.300d.zip

from gensim.scripts.glove2word2vec import glove2word2vec

# glove_input_file = r"/content/glove.6B.300d.txt"
glove_input_file = r"/content/glove.42B.300d.txt"
glove_output_file = r"word2vec.txt"

glove2word2vec(glove_input_file, glove_output_file)

from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format(glove_output_file, binary=False)

model.similarity('go','went')

"""print_related function based on cosine similarity"""

def print_related(word = 'india', topn = 7) : 
  try : 
    word = word.lower()
    top_wd = model.most_similar(word, topn = topn)
    print(f"{word} related Word : ")
    for itm in top_wd : 
      print(f"{itm[0]} {round(itm[1]*100, 2)}%")
  except : 
    print(f"{word} has spelled mistakes or does not exist...")

print_related()

"""Surprisingly enough:

vector(“France”) - vector("Paris") = answer_vector - vector("Rome")

Therefore:

vector(“France”) - vector("Paris") + vector("Rome") = answer_vector

We’ll look for words close to answer_vector. The answer_vector won’t match “Italy” exactly but it should be close.
"""

def print_analogy(word1 = 'king', word2 = 'man', word3 = 'woman', topn = 1, percentage = False) : 
  try : 
    word1 = word1.lower()
    word2 = word2.lower()
    word3 = word3.lower()
    top_wd = model.most_similar(positive=[word2, word3], negative=[word1], topn = topn)
    print(f"{word1} : {word2} :: {word3} : {top_wd[0][0]}")
    if percentage : 
      for itm in top_wd : 
        print(f"{itm[0]} - {round(itm[1]*100, 2)}%")
  except : 
    print(f"{word1} | {word2} | {word3} have spelled mistake or does not exist...")

# 'Paris', 'France', 'Rome'
# 'man', 'king', 'woman'
# 'walk', 'walked' , 'go'
# 'do', 'done' , 'go'
# 'quick', 'quickest' , 'far'

print_analogy('Paris', 'France', 'Rome', 5, percentage=True)

"""Not-related word found in a group of words is called ODD
exectly same functon doesnt_match
"""

def doesnt_match(words) : 
  try : 
    words = " ".join(words).lower().split(" ")
    print(model.doesnt_match(words))
  except : 
    print(f"{words} have spelled mistake or does not exist...")
  # print(model.doesnt_match("breakfast robot dinner lunch".split()))



"""similarity function is return how much percentage related to each other words"""

def similarity(word1 = 'woman', word2 = 'man') : 
  try : 
    word1 = word1.lower()
    word2 = word2.lower()
    print(f"{word1} and {word2} are similar to {round(model.similarity(word1, word2)*100, 2 )} %")
  except : 
    print(f"{word1} | {word2} have spelled mistake or does not exist...")

similarity()

model.doesnt_match(['tea','water', 'mango'])

# spain called although arms roots
# print_related("spain")

print_related('roots')

# 'Paris', 'France', 'Rome'
# 'man', 'king', 'woman'
# 'walk', 'walked' , 'go'
# 'do', 'done' , 'go'
# 'quick', 'quickest' , 'far'
# print_analogy('Paris', 'France', 'Rome', 5, percentage=True)

print_analogy('quick', 'quickest' , 'far', 5, percentage=True)

# 'breakfast', 'robot', 'dinner', 'lunch'
# 'Spain', 'Russia', 'Canada', 'Africa'
# 'banana', 'apple', 'rice', 'grape'
# 'car', 'plane', 'road', 'train'
# 'king','queen', 'prince', 'man'
# doesnt_match([])

doesnt_match(['car', 'plane', 'road', 'train'])

# man woman
# boy girl
# cat dog
# india pakishtan
# similarity('king', 'queen')

similarity('you', 'me')

